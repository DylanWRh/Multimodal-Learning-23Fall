{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9gCNVvrkA6d"
      },
      "source": [
        "# Assignment 2\n",
        "## Section 1 [4 pts]\n",
        "\n",
        "In the last assignment, you have implemented a simple neural network with fully connected layers to fit a polynomial function.\n",
        "In this section, let's try something more challenging and realistic: implementing a CNN model for handwritten digit recognition.\n",
        "\n",
        "This assignment will make you familiar with\n",
        "1. loading and preprocessing data using built-in function\n",
        "2. constructing a simple CNN model with PyTorch\n",
        "3. the training and testing pipeline\n",
        "\n",
        "You might find this tutorial useful: https://pytorch.org/tutorials/beginner/basics/intro.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:06:57.475537700Z",
          "start_time": "2023-11-02T14:06:57.461541100Z"
        },
        "id": "N7OzMyzk1WEc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wrh/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Import dependencies.\n",
        "import time\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:06:58.358205500Z",
          "start_time": "2023-11-02T14:06:58.330210100Z"
        },
        "id": "Xe6caWN_1WEd"
      },
      "outputs": [],
      "source": [
        "# Set up your device\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:06:59.288906900Z",
          "start_time": "2023-11-02T14:06:59.268913500Z"
        },
        "id": "PbWR_5nB1WEd"
      },
      "outputs": [],
      "source": [
        "# Random seed helps you get the same result everytime you run the code.\n",
        "# Keeping the same random seed is crucial to ensure that your result is reproducible.\n",
        "seed = 1008\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    # TODO: Opening question: what does this line do?\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3MQkukl1WEd"
      },
      "source": [
        "## 1. Data: MNIST [1 pt]\n",
        "#### Use the built-in MNIST dataset in $\\texttt{torchvision.datasets}$. It offers automatic data downloading and preprocessing.\n",
        "#### Use $\\texttt{torch.utils.data.DataLoader}$ to load data in batches.\n",
        "\n",
        "MNIST dataset is a dataset of handwritten digits.\n",
        "It is one of the most commonly used vision dataset for testing machine learning algorithms.\n",
        "It has been built-in in PyTorch, so you can easily load it using $\\texttt{torchvision.datasets}$.\n",
        "\n",
        "The number of categories of this dataset is 10.\n",
        "The shape of image in MNIST dataset is (28, 28, 1), each dimension representing height, width and channel respectively.\n",
        "\n",
        "The normalization parameters we will use is (0.1307, 0.3081)\n",
        "\n",
        "More details please refer to http://yann.lecun.com/exdb/mnist/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_3sASfx1WEe"
      },
      "source": [
        "### 1.1. Load Training Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:07:02.385655Z",
          "start_time": "2023-11-02T14:07:02.350656900Z"
        },
        "id": "UxI1Bov71WEe"
      },
      "outputs": [],
      "source": [
        "# DataLoader helps you load data from dataset and yield a batch of data in every iteration.\n",
        "# Load the MNIST training set with batch size 128\n",
        "# For training data, apply data shuffling and normalization\n",
        "batch_size = 128\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./',\n",
        "    # TODO: fill in the parameters to build the training set\n",
        "    train=True,\n",
        "    transform=transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=0.1307, std=0.3081)]),\n",
        "    download=True\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    # TODO: fill in the parameters to build the training loader\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpWxQFFx1WEe"
      },
      "source": [
        "### 1.2. Load Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:07:03.702966500Z",
          "start_time": "2023-11-02T14:07:03.680829300Z"
        },
        "id": "zu55m4_h1WEf"
      },
      "outputs": [],
      "source": [
        "# Load the MNIST test set with batch size 128, apply normalization\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./',\n",
        "    # TODO: fill in the parameters to build the test set\n",
        "    train=False,\n",
        "    transform=transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=0.1307, std=0.3081)]),\n",
        "    download=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    # TODO: fill in the parameters to build the test loader\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:07:04.376727300Z",
          "start_time": "2023-11-02T14:07:04.338732900Z"
        },
        "id": "KK5KmnTyjIqV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "# Check the shape of data and label\n",
        "for img_batch, label_batch in train_loader:\n",
        "    print(img_batch.shape)\n",
        "    print(label_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OspQSBSz1WEf"
      },
      "source": [
        "## 2. Models [1 pt]\n",
        "#### You are going to define two convolutional neural networks which are trained to classify MNIST digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqEs-i9H1WEg"
      },
      "source": [
        "### 2.1. CNN without Batch Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:07:07.460437200Z",
          "start_time": "2023-11-02T14:07:07.435332Z"
        },
        "id": "QyMaix4F1WEg"
      },
      "outputs": [],
      "source": [
        "class NetWithoutBatchNorm(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(NetWithoutBatchNorm, self).__init__()\n",
        "        # TODO: Fill in the values below that make this network valid for MNIST data\n",
        "        # Hint: to make sure these, you may calculate the shape of x of every line in the forward.\n",
        "        conv2_in_channels = 20\n",
        "        fc1_in_features = 800\n",
        "        fc2_in_features = 500\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=20, kernel_size=5, stride=1)\n",
        "        self.activation1 = nn.ReLU()\n",
        "        self.pooling1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=conv2_in_channels, out_channels=50, kernel_size=5, stride=1)\n",
        "        self.activation2 = nn.ReLU()\n",
        "        self.pooling2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=500)\n",
        "        self.activation_fc = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=fc2_in_features, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation1(self.conv1(x))\n",
        "        x = self.pooling1(x)\n",
        "        x = self.activation2(self.conv2(x))\n",
        "        x = self.pooling2(x)\n",
        "        # TODO: transform the shape of x to fit the fully connected layer\n",
        "        x = x.view(-1, 800)\n",
        "        x = self.activation_fc(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFKJlgu81WEh"
      },
      "source": [
        "### 2.2. CNN with Batch Norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:07:08.901831Z",
          "start_time": "2023-11-02T14:07:08.878835400Z"
        },
        "id": "b-0DOwV31WEh"
      },
      "outputs": [],
      "source": [
        "# Please note that this model inheritance from NetWithoutBatchNorm.\n",
        "class NetWithBatchNorm(NetWithoutBatchNorm):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(NetWithBatchNorm, self).__init__(in_channels, num_classes)\n",
        "        # TODO: Fill in the values below that make this network valid for MNIST data\n",
        "        # Hint: to make sure these, you may calculate the shape of x of every line in the forward.\n",
        "        conv1_bn_size = 20\n",
        "        conv2_bn_size = 50\n",
        "        self.conv1_bn = nn.BatchNorm2d(conv1_bn_size)\n",
        "        self.conv2_bn = nn.BatchNorm2d(conv2_bn_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: follow the forward function in 2.1, add batch norm layers\n",
        "        x = self.activation1(self.conv1(x))\n",
        "        x = self.conv1_bn(x)\n",
        "        x = self.pooling1(x)\n",
        "        x = self.activation2(self.conv2(x))\n",
        "        x = self.conv2_bn(x)\n",
        "        x = self.pooling2(x)\n",
        "        x = x.view(-1, 800)\n",
        "        x = self.activation_fc(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0psJkBxm1WEh"
      },
      "source": [
        "## 3. Training & Evaluation [1 pt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFY-_pb71WEi"
      },
      "source": [
        "### 3.1. Define training method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:07:11.813884400Z",
          "start_time": "2023-11-02T14:07:11.801883700Z"
        },
        "id": "5xL7MVll1WEi"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, data_loader, optimizer, loss_fn, epoch_cnt, log_interval = 100):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    # Loop through data points\n",
        "    for batch_idx, (data, label) in enumerate(data_loader):\n",
        "        # TODO: Send data and label to device\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "        # TODO: Zero out the optimizer\n",
        "        optimizer.zero_grad()\n",
        "        # TODO: Forward pass\n",
        "        logits = model(data)\n",
        "        # TODO: Compute the loss\n",
        "        loss = loss_fn(logits, label)\n",
        "        # TODO: Backpropagation\n",
        "        loss.backward()\n",
        "        # TODO: Update parameters\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch_cnt, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZygvwvvW1WEi"
      },
      "source": [
        "### 3.2. Define test method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:15:20.456462Z",
          "start_time": "2023-11-02T14:15:20.444828800Z"
        },
        "id": "Xix5BhkV1WEi"
      },
      "outputs": [],
      "source": [
        "# Define test method\n",
        "def test(model, data_loader, loss_fn):\n",
        "    # TODO: Set model to evaluation mode\n",
        "    # Variable for the total loss\n",
        "    test_loss = 0\n",
        "    # Counter for the correct predictions\n",
        "    num_correct = 0\n",
        "    # During evaluation, disable gradient tracking for efficiency\n",
        "    with torch.no_grad():\n",
        "        # Loop through data points\n",
        "        for data, label in data_loader:\n",
        "            # TODO: Send data to device\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            # TODO: Forward pass\n",
        "            logits = model(data)\n",
        "            # TODO: Compute loss and add to total test_loss\n",
        "            test_loss += loss_fn(logits, label)\n",
        "            # TODO: Get predictions from the model for each data point\n",
        "            correct = (logits.argmax(1) == label).sum()\n",
        "            # TODO: Add number of correct predictions to total num_correct\n",
        "            num_correct += correct\n",
        "    # TODO: Compute the average loss on each data point\n",
        "    avg_test_loss = test_loss / len(data_loader.dataset)\n",
        "    print('-- Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
        "        100. * num_correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li3a435x1WEi"
      },
      "source": [
        "### 3.3 Train the network without batch norm and the network with batch norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:20:23.830932300Z",
          "start_time": "2023-11-02T14:18:13.894231Z"
        },
        "colab": {
          "background_save": true
        },
        "id": "3J_u4HI41WEj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304002\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.527209\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.298741\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.299091\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.210748\n",
            "-- Test set: Average loss: 0.1491, Accuracy: 9562/10000 (96%)\n",
            "Training without batch norm epoch 1 finished, time cost: 0:00:07.043003\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.189302\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.206976\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.106637\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.156122\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.102701\n",
            "-- Test set: Average loss: 0.0895, Accuracy: 9718/10000 (97%)\n",
            "Training without batch norm epoch 2 finished, time cost: 0:00:05.623758\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.054722\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.083246\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.096126\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.053714\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.166698\n",
            "-- Test set: Average loss: 0.0671, Accuracy: 9801/10000 (98%)\n",
            "Training without batch norm epoch 3 finished, time cost: 0:00:05.079337\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.063715\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.067890\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.075059\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.043043\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.061930\n",
            "-- Test set: Average loss: 0.0592, Accuracy: 9826/10000 (98%)\n",
            "Training without batch norm epoch 4 finished, time cost: 0:00:05.235083\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.110281\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.073617\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.029867\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.022575\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.043808\n",
            "-- Test set: Average loss: 0.0461, Accuracy: 9867/10000 (99%)\n",
            "Training without batch norm epoch 5 finished, time cost: 0:00:05.157286\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.007443\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.070717\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.030710\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.053309\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.074420\n",
            "-- Test set: Average loss: 0.0472, Accuracy: 9853/10000 (99%)\n",
            "Training without batch norm epoch 6 finished, time cost: 0:00:05.216073\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.047701\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.038496\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.031441\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.070831\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.064729\n",
            "-- Test set: Average loss: 0.0411, Accuracy: 9876/10000 (99%)\n",
            "Training without batch norm epoch 7 finished, time cost: 0:00:05.004722\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.040832\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.009734\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.044609\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.053118\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.021487\n",
            "-- Test set: Average loss: 0.0358, Accuracy: 9890/10000 (99%)\n",
            "Training without batch norm epoch 8 finished, time cost: 0:00:04.871210\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.049564\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.063323\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.003117\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.020977\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.032964\n",
            "-- Test set: Average loss: 0.0370, Accuracy: 9887/10000 (99%)\n",
            "Training without batch norm epoch 9 finished, time cost: 0:00:04.679795\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.048428\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.015093\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.070704\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.033757\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.027357\n",
            "-- Test set: Average loss: 0.0322, Accuracy: 9888/10000 (99%)\n",
            "Training without batch norm epoch 10 finished, time cost: 0:00:04.674706\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define the network without batch norm and sent it to device\n",
        "net_without_bn = NetWithoutBatchNorm(in_channels=1, num_classes=10).to(device)\n",
        "# TODO: Define an SGD optimizer with learning rate of 1e-2 and momentum of 0.5\n",
        "sgd_optimizer = optim.SGD(net_without_bn.parameters(), lr=1e-2, momentum=0.5)\n",
        "# TODO: Define a cross entropy loss function, reduction='mean' for training\n",
        "loss_fn_train = nn.CrossEntropyLoss(reduction='mean')\n",
        "# TODO: Define a cross entropy loss function, reduction='sum' for testing\n",
        "loss_fn_test = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "# Training loop with 10 epochs\n",
        "for epoch in range(1, 10 + 1):\n",
        "    start_time = time.time()\n",
        "    # TODO: call the train_one_epoch function to train the model\n",
        "    train_one_epoch(\n",
        "        model=net_without_bn,\n",
        "        data_loader=train_loader,\n",
        "        optimizer=sgd_optimizer,\n",
        "        loss_fn=loss_fn_train,\n",
        "        epoch_cnt=epoch,\n",
        "    )\n",
        "    # TODO: call the test function to test the model\n",
        "    test(\n",
        "        model=net_without_bn,\n",
        "        data_loader=test_loader,\n",
        "        loss_fn=loss_fn_test,\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    time_cost = datetime.timedelta(seconds=end_time - start_time)\n",
        "    print(\"Training without batch norm epoch {} finished, time cost: {}\".format(epoch, time_cost))\n",
        "    print('------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-11-02T14:23:01.465190400Z",
          "start_time": "2023-11-02T14:20:42.593150Z"
        },
        "id": "bLzzlHncjIqY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.373857\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.235528\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.097148\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.122791\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.116315\n",
            "-- Test set: Average loss: 0.0674, Accuracy: 9802/10000 (98%)\n",
            "Training with batch norm epoch 1 finished, time cost: 0:00:04.975342\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.038592\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.087226\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.047951\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.068233\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.039718\n",
            "-- Test set: Average loss: 0.0470, Accuracy: 9861/10000 (99%)\n",
            "Training with batch norm epoch 2 finished, time cost: 0:00:04.798969\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.069797\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.039659\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.061331\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.053328\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.032559\n",
            "-- Test set: Average loss: 0.0350, Accuracy: 9892/10000 (99%)\n",
            "Training with batch norm epoch 3 finished, time cost: 0:00:04.693920\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.047162\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.028712\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.019067\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.063877\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.052113\n",
            "-- Test set: Average loss: 0.0305, Accuracy: 9896/10000 (99%)\n",
            "Training with batch norm epoch 4 finished, time cost: 0:00:04.640055\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.016867\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.019851\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.011350\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.031261\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.021443\n",
            "-- Test set: Average loss: 0.0302, Accuracy: 9898/10000 (99%)\n",
            "Training with batch norm epoch 5 finished, time cost: 0:00:05.044377\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.008269\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.004303\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.036913\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.024494\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.016347\n",
            "-- Test set: Average loss: 0.0282, Accuracy: 9903/10000 (99%)\n",
            "Training with batch norm epoch 6 finished, time cost: 0:00:05.007377\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.020570\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.012958\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.002409\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.018575\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.003992\n",
            "-- Test set: Average loss: 0.0275, Accuracy: 9909/10000 (99%)\n",
            "Training with batch norm epoch 7 finished, time cost: 0:00:05.014483\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.053325\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.016450\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.008348\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.010579\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.039537\n",
            "-- Test set: Average loss: 0.0261, Accuracy: 9909/10000 (99%)\n",
            "Training with batch norm epoch 8 finished, time cost: 0:00:05.095041\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.007083\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.007417\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.022195\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.007962\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004851\n",
            "-- Test set: Average loss: 0.0268, Accuracy: 9907/10000 (99%)\n",
            "Training with batch norm epoch 9 finished, time cost: 0:00:05.436524\n",
            "------------------------------------------------------------\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.006452\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.013198\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.010103\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.006577\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.005170\n",
            "-- Test set: Average loss: 0.0253, Accuracy: 9917/10000 (99%)\n",
            "Training with batch norm epoch 10 finished, time cost: 0:00:05.251182\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# TODO: Define the network with batch norm and sent it to device\n",
        "net_with_bn = NetWithBatchNorm(in_channels=1, num_classes=10).to(device)\n",
        "# TODO: Define an SGD optimizer with learning rate of 1e-2 and momentum of 0.5\n",
        "sgd_optimizer = optim.SGD(net_with_bn.parameters(), lr=1e-2, momentum=0.5)\n",
        "\n",
        "# Training loop with 10 epochs\n",
        "for epoch in range(1, 10 + 1):\n",
        "    start_time = time.time()\n",
        "    # TODO: call the train_one_epoch function to train the model\n",
        "    train_one_epoch(\n",
        "        model=net_with_bn,\n",
        "        data_loader=train_loader,\n",
        "        optimizer=sgd_optimizer,\n",
        "        loss_fn=loss_fn_train,\n",
        "        epoch_cnt=epoch,\n",
        "    )\n",
        "    # TODO: call the test function to test the model\n",
        "    test(\n",
        "        model=net_with_bn,\n",
        "        data_loader=test_loader,\n",
        "        loss_fn=loss_fn_test,\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    time_cost = datetime.timedelta(seconds=end_time - start_time)\n",
        "    print(\"Training with batch norm epoch {} finished, time cost: {}\".format(epoch, time_cost))\n",
        "    print('------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb9wYwme1WEj"
      },
      "source": [
        "## 4. Opening questions [1pt]\n",
        "### 4.1 In random seed setting section, what does $\\texttt{torch.backends.cudnn.benchmark}$ determines? Why do we set it to $\\texttt{False}$?\n",
        "### 4.2 When defining the network, where do you add the batch norm layers? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlY11BPA1WEj"
      },
      "source": [
        "Answers:\n",
        "\n",
        "4.1 若将`torch.backends.cudnn.benchmark`设置为`True`，则会使得`torch`的底层库`cuDNN`衡量多个卷积算法的速度并选择最快的；将其设置为`False`的原因是减少算法的不确定性，更好地控制除了有无batch norm以外的变量\n",
        "\n",
        "4.2 加载activation层之后，可以使得下一次卷积输入的值是normalize过的，在相同的尺度范围内，有利于卷积层的学习"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jovnEJ6D7rBN"
      },
      "source": [
        "## Section 2 [6 pts]\n",
        "\n",
        "You will tackle with a sentiment classification task using LSTM model and attention mechanism in this assigment.\n",
        "\n",
        "## 1. Dependencies\n",
        "Please make sure that you are using **GPU** to accelarate computation.\n",
        "You can run with Google Colab if you don't have.\n",
        "Colab FAQ: https://research.google.com/colaboratory/faq.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVAMnQR8xflq"
      },
      "source": [
        "### 1.1. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6Kk2dEh8--MH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import collections\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MdjDF7WRF7UU"
      },
      "outputs": [],
      "source": [
        "# Set up your device\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
        "\n",
        "# The assertion is to make sure GPU is available\n",
        "assert cuda == True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wXkKqcxiO6b3"
      },
      "outputs": [],
      "source": [
        "# Set up random seed to 1331. Do not change the random seed.\n",
        "# Yes, these are all necessary when you run experiments!\n",
        "seed = 1331\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLvOvdMbO9Qn"
      },
      "source": [
        "### 1.2. Data\n",
        "The script below will download the required sentiment analysis data.\n",
        "\n",
        "If you use Google Colab, data folder will be visible in the Colab file-explorer pane, which is loacted at left side of the page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxNT5mN-rHGD",
        "outputId": "828d7780-0610-4ea8-a1ab-e00e6c934dab"
      },
      "outputs": [],
      "source": [
        "# !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1y-_KAx7oLqjzOnyXeZvt0tp9WC2ZgyUi' -O data.zip\n",
        "# !unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7teo5qvRPnYP"
      },
      "source": [
        "### 1.3. Corpus\n",
        "Glove will be used as the word embedding tool in this assigment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "34dba6GT_EhP",
        "outputId": "ee173258-f984-475f-fb2f-e5de20f3209a"
      },
      "outputs": [],
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPEcJ_L8QqZu"
      },
      "source": [
        "## 2. Preprocess [1.5 pt]\n",
        "Preprocess data, then construct dataloader and vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXPeeetKXvWh"
      },
      "source": [
        "### 2.1. Load Glove pretrained word embedding. [0.5 pt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nADNNIT11U-P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "400000it [00:05, 69022.78it/s]\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "def get_glove_embedding():\n",
        "    filename = 'glove.6B.100d.txt'\n",
        "    glove_embedding = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in tqdm(f):\n",
        "            vals = line.rstrip().split(' ')\n",
        "            glove_embedding.append([float(x) for x in vals[1:]])\n",
        "    feature_size = len(glove_embedding[0])\n",
        "    unk_feature = torch.randn(1, feature_size)\n",
        "    pad_feature = torch.randn(1, feature_size)\n",
        "    glove_embedding = torch.Tensor(glove_embedding)\n",
        "    glove_embedding = torch.cat([glove_embedding, unk_feature, pad_feature])\n",
        "    # print(glove_embedding.shape)\n",
        "    glove_embedding_nn = nn.Embedding(*glove_embedding.shape)\n",
        "    glove_embedding_nn.weight = torch.nn.Parameter(glove_embedding)\n",
        "    # glove_embedding_nn.weight.requires_grad = False\n",
        "    return glove_embedding_nn\n",
        "\n",
        "glove_embedding = get_glove_embedding()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOkdvhoeRHu3"
      },
      "source": [
        "### 2.2. Construct your own vocabulary without other corpus. [0.5 pt]\n",
        "Hint: You should construct a vocabulary to map the word to index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FXgxiqptBsAK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "400000it [00:01, 375344.75it/s]\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "vocabs = []\n",
        "filename = 'glove.6B.100d.txt'\n",
        "with open(filename, 'r') as f:\n",
        "    for line in tqdm(f):\n",
        "        word = line.rstrip().split(' ')[0]\n",
        "        vocabs.append(word)\n",
        "vocabs.append('<UNK>')\n",
        "vocabs.append('<PAD>')\n",
        "word2idx = {word:idx for idx, word in enumerate(vocabs)}\n",
        "vocab_size = len(word2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbw-0N7KQ6B2"
      },
      "source": [
        "### 2.3. Load data [0.5 pt]\n",
        "Load data and construct dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "p5zh-qrS_aAl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "45615it [00:00, 92985.20it/s]\n",
            "45615it [00:00, 4515534.03it/s]\n",
            "2000it [00:00, 36481.88it/s]\n",
            "2000it [00:00, 4021384.47it/s]\n",
            "12284it [00:00, 99621.28it/s]\n",
            "12284it [00:00, 4729902.72it/s]\n"
          ]
        }
      ],
      "source": [
        "data_dir = 'sentiment'\n",
        "# TODO\n",
        "\n",
        "def clean_str(s):\n",
        "    s = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", s)\n",
        "    s = re.sub(r\"\\'s\", \" \\'s\", s)  \n",
        "    s = re.sub(r\"\\'ve\", \" \\'ve\", s)\n",
        "    s = re.sub(r\"n\\'t\", \" n\\'t\", s)\n",
        "    s = re.sub(r\"\\'re\", \" \\'re\", s)\n",
        "    s = re.sub(r\"\\'d\", \" \\'d\", s) \n",
        "    s = re.sub(r\"\\'ll\", \" \\'ll\", s)\n",
        "    s = re.sub(r\"\\'m\", \" \\'m\", s) \n",
        "    s = re.sub(r\",\", \" , \", s)\n",
        "    s = re.sub(r\"!\", \" ! \", s)\n",
        "    s = re.sub(r\"\\(\", \" ( \", s)\n",
        "    s = re.sub(r\"\\)\", \" ) \", s)\n",
        "    s = re.sub(r\"\\?\", \" ? \", s)\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
        "    return s.strip().lower()\n",
        "\n",
        "def build_dataloader(path, max_per_sentence=50, batch_size=128, shuffle=False):\n",
        "    assert path in ['train', 'test', 'val']\n",
        "    text_path = os.path.join(data_dir, path+'_text.txt')\n",
        "    label_path = os.path.join(data_dir, path+'_labels.txt')\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(text_path, 'r') as f:\n",
        "        for line in tqdm(f):\n",
        "            texts.append(clean_str(line).split(' '))\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in tqdm(f):\n",
        "            labels.append(int(line.strip()))\n",
        "    sentences = []\n",
        "    for text in texts:\n",
        "        text += ['<PAD>'] * (max_per_sentence - len(text))\n",
        "        unk_idx = word2idx['<UNK>']\n",
        "        text2idx = [word2idx.get(word, unk_idx) for word in text][:max_per_sentence]\n",
        "        sentences.append(text2idx)\n",
        "    sentences = torch.tensor(sentences, dtype=torch.long)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    text_dataset = TensorDataset(sentences, labels)\n",
        "    text_dataloader = DataLoader(dataset=text_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return text_dataloader\n",
        "train_loader = build_dataloader('train', shuffle=True)\n",
        "val_loader = build_dataloader('val')\n",
        "test_loader = build_dataloader('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOfFQoClxPTl"
      },
      "source": [
        "## 3. Model [1.5 pt]\n",
        "Bidirectional LSTM and attention mechanism will be used in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDPbq8VmEvdB"
      },
      "source": [
        "### 3.1. Model Zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vTyWAUr81oR9"
      },
      "outputs": [],
      "source": [
        "class BiRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
        "        super(BiRNN, self).__init__()\n",
        "        # TODO: load pretrained embedding\n",
        "        ...\n",
        "        self.embedding = pretrained_embedding \\\n",
        "            if pretrained_embedding is not None \\\n",
        "            else nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        self.decoder = nn.Sequential(nn.Linear(4 * num_hiddens, num_hiddens), nn.Linear(num_hiddens, 3))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        self.encoder.flatten_parameters()\n",
        "        outputs, _ = self.encoder(embeddings)\n",
        "        encoding = torch.cat((outputs[:,0,:], outputs[:,-1,:]), dim=1)\n",
        "        outs = self.decoder(encoding)\n",
        "        return outs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WBXSkgb41oVF"
      },
      "outputs": [],
      "source": [
        "class BiRNN_attention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=None, **kwargs):\n",
        "        super(BiRNN_attention, self).__init__()\n",
        "        # TODO: load pretrained embedding\n",
        "        ...\n",
        "        self.embedding = pretrained_embedding \\\n",
        "            if pretrained_embedding is not None \\\n",
        "            else nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        self.weight_W = nn.Parameter(torch.Tensor(2 * num_hiddens, 2 * num_hiddens))\n",
        "        self.weight_proj = nn.Parameter(torch.Tensor(2 * num_hiddens, 1))\n",
        "\n",
        "        self.decoder = nn.Sequential(nn.Linear(2 * num_hiddens, num_hiddens), nn.Linear(num_hiddens, 3))\n",
        "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        mask = 1 - torch.clamp(inputs, min=0, max=1)\n",
        "        embeddings = self.embedding(inputs)\n",
        "        states, hidden = self.encoder(embeddings.permute([0, 1, 2]))\n",
        "        u = torch.tanh(torch.matmul(states, self.weight_W))\n",
        "        att = torch.matmul(u, self.weight_proj)\n",
        "        att = att + mask.unsqueeze(2) * -1e7\n",
        "        att_score = F.softmax(att, dim=1)\n",
        "        scored_x = states * att_score\n",
        "        encoding = torch.sum(scored_x, dim=1)\n",
        "        outputs = self.decoder(encoding)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HE235LrErqv"
      },
      "source": [
        "## 4. Training [3 pt]\n",
        "You should train two models above with Glove pretrained word embedding and random initialized word embedding.\n",
        "\n",
        "Evaluation on the validation set and print out accuracy after training one epoch is required.\n",
        "\n",
        "You can tune some parameters and try different techniques, such as learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "atAtYp4Z_z0t"
      },
      "outputs": [],
      "source": [
        "# Training Entry Point, you can add arbitrary parameters to replace **kwargs if you need.\n",
        "def test(model, data_loader, loss_fn, phase='val'):\n",
        "    assert phase in ['val', 'test']\n",
        "    test_loss = 0\n",
        "    num_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, label in data_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            logits = model(data)\n",
        "            test_loss += loss_fn(logits, label)\n",
        "            correct = (logits.argmax(1) == label).sum()\n",
        "            num_correct += correct\n",
        "    avg_test_loss = test_loss / len(data_loader.dataset)\n",
        "    print('-- {} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        phase, avg_test_loss, num_correct, len(data_loader.dataset),\n",
        "        100. * num_correct / len(data_loader.dataset)))\n",
        "\n",
        "def train(network_class, pretrained_embedding=None, **kwargs):\n",
        "    # TODOembed_size, num_hiddens, num_layers, pretrained_embedding=None\n",
        "    # vocab_size defined in section 2.2 constructing vocabulary\n",
        "    embed_size = kwargs.get('embed_size', 100)\n",
        "    num_hiddens = kwargs.get('num_hiddens', 100)\n",
        "    num_layers = kwargs.get('num_layers', 1)\n",
        "    epochs = kwargs.get('epochs', 30)\n",
        "\n",
        "    model = network_class(vocab_size, embed_size, num_hiddens, num_layers, pretrained_embedding=pretrained_embedding)\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-3)\n",
        "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        for data, label in tqdm(train_loader):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(data)\n",
        "            loss = loss_fn(logits, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        print('Train Epoch: {}\\tLoss: {:.6f}'.format(i+1, loss.item() / len(data)))\n",
        "        if (i+1) % 10 == 0:\n",
        "            test(model, val_loader, loss_fn, phase='val')\n",
        "    test(model, test_loader, loss_fn, phase='test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "j3gbyRUXEPZP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 90.27it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tLoss: 0.019827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tLoss: 0.018386\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 106.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 3\tLoss: 0.015222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 4\tLoss: 0.014736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 5\tLoss: 0.013477\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 6\tLoss: 0.014448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 7\tLoss: 0.016555\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 8\tLoss: 0.017438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 9\tLoss: 0.017286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 10\tLoss: 0.016477\n",
            "-- val set: Average loss: 0.0063, Accuracy: 1242/2000 (62%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 11\tLoss: 0.013527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 12\tLoss: 0.014461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 13\tLoss: 0.017303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 14\tLoss: 0.017664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 15\tLoss: 0.019378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 16\tLoss: 0.016028\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 17\tLoss: 0.020346\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 18\tLoss: 0.016732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 19\tLoss: 0.016181\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 20\tLoss: 0.017878\n",
            "-- val set: Average loss: 0.0061, Accuracy: 1280/2000 (64%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 21\tLoss: 0.015485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 22\tLoss: 0.014759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 23\tLoss: 0.016751\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 24\tLoss: 0.016489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 25\tLoss: 0.014659\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 26\tLoss: 0.014470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 27\tLoss: 0.016033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 28\tLoss: 0.015515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 29\tLoss: 0.018770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 30\tLoss: 0.012407\n",
            "-- val set: Average loss: 0.0061, Accuracy: 1288/2000 (64%)\n",
            "-- test set: Average loss: 0.0061, Accuracy: 7875/12284 (64%)\n"
          ]
        }
      ],
      "source": [
        "# Train BiRNN with Glove pretrained word embedding\n",
        "# TODO\n",
        "train(BiRNN, pretrained_embedding=glove_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6rXFCb56GBOK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tLoss: 0.021657\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tLoss: 0.020470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 3\tLoss: 0.020289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 4\tLoss: 0.019367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 5\tLoss: 0.022210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 6\tLoss: 0.019365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 108.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 7\tLoss: 0.017486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 8\tLoss: 0.019598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 9\tLoss: 0.020165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 10\tLoss: 0.020129\n",
            "-- val set: Average loss: 0.0074, Accuracy: 1113/2000 (56%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 11\tLoss: 0.019909\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 12\tLoss: 0.017175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 13\tLoss: 0.018298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 14\tLoss: 0.018032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 15\tLoss: 0.018362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 16\tLoss: 0.017259\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 17\tLoss: 0.015649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 18\tLoss: 0.018744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 19\tLoss: 0.017507\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 20\tLoss: 0.018774\n",
            "-- val set: Average loss: 0.0072, Accuracy: 1171/2000 (59%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 21\tLoss: 0.016941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 22\tLoss: 0.017106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 23\tLoss: 0.017442\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 24\tLoss: 0.017863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 25\tLoss: 0.020233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 26\tLoss: 0.015128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 104.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 27\tLoss: 0.021037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 28\tLoss: 0.017929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 105.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 29\tLoss: 0.014678\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 106.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 30\tLoss: 0.017414\n",
            "-- val set: Average loss: 0.0067, Accuracy: 1238/2000 (62%)\n",
            "-- test set: Average loss: 0.0077, Accuracy: 6442/12284 (52%)\n"
          ]
        }
      ],
      "source": [
        "# Train BiRNN without pretrained word embedding\n",
        "# TODO\n",
        "train(BiRNN, pretrained_embedding=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "aeetQ2uZGBXf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tLoss: 0.020929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.15it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tLoss: 0.019154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 3\tLoss: 0.018812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 4\tLoss: 0.017853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 5\tLoss: 0.014807\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 6\tLoss: 0.014039\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 7\tLoss: 0.014462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 8\tLoss: 0.017660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 103.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 9\tLoss: 0.019922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 10\tLoss: 0.015511\n",
            "-- val set: Average loss: 0.0061, Accuracy: 1311/2000 (66%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 11\tLoss: 0.018278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 12\tLoss: 0.016764\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 13\tLoss: 0.014735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 14\tLoss: 0.012956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 15\tLoss: 0.014267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 99.10it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 16\tLoss: 0.015883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 102.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 17\tLoss: 0.016039\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 18\tLoss: 0.017624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 19\tLoss: 0.013295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 20\tLoss: 0.017065\n",
            "-- val set: Average loss: 0.0060, Accuracy: 1322/2000 (66%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 21\tLoss: 0.014910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 22\tLoss: 0.017940\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 23\tLoss: 0.013310\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.28it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 24\tLoss: 0.015502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.92it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 25\tLoss: 0.013002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.53it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 26\tLoss: 0.017011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.34it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 27\tLoss: 0.013705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 99.79it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 28\tLoss: 0.015438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.67it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 29\tLoss: 0.017534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 30\tLoss: 0.016359\n",
            "-- val set: Average loss: 0.0059, Accuracy: 1321/2000 (66%)\n",
            "-- test set: Average loss: 0.0061, Accuracy: 7821/12284 (64%)\n"
          ]
        }
      ],
      "source": [
        "# Train BiRNN_attention with Glove pretrained embedding\n",
        "# TODO\n",
        "train(BiRNN_attention, pretrained_embedding=glove_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uiRSuAy0GBdJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 99.54it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tLoss: 0.020111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tLoss: 0.022807\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 99.05it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 3\tLoss: 0.018743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 99.13it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 4\tLoss: 0.020329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 99.41it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 5\tLoss: 0.021172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 6\tLoss: 0.019091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 95.83it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 7\tLoss: 0.022365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 95.56it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 8\tLoss: 0.020068\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.64it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 9\tLoss: 0.017855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 96.51it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 10\tLoss: 0.017307\n",
            "-- val set: Average loss: 0.0073, Accuracy: 1124/2000 (56%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.19it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 11\tLoss: 0.020739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.36it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 12\tLoss: 0.022198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 99.50it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 13\tLoss: 0.018029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 99.49it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 14\tLoss: 0.017343\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 101.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 15\tLoss: 0.016060\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 100.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 16\tLoss: 0.016443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.28it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 17\tLoss: 0.018839\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.77it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 18\tLoss: 0.017356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 99.66it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 19\tLoss: 0.018465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.80it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 20\tLoss: 0.018799\n",
            "-- val set: Average loss: 0.0072, Accuracy: 1143/2000 (57%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.46it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 21\tLoss: 0.018667\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.22it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 22\tLoss: 0.018082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.92it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 23\tLoss: 0.018440\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.93it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 24\tLoss: 0.016665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.96it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 25\tLoss: 0.018928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.57it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 26\tLoss: 0.019831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.10it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 27\tLoss: 0.017644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.81it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 28\tLoss: 0.018753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 98.80it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 29\tLoss: 0.019202\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 357/357 [00:03<00:00, 97.95it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 30\tLoss: 0.015797\n",
            "-- val set: Average loss: 0.0072, Accuracy: 1143/2000 (57%)\n",
            "-- test set: Average loss: 0.0084, Accuracy: 6245/12284 (51%)\n"
          ]
        }
      ],
      "source": [
        "# Train BiRNN_attention without pretrained word embedding\n",
        "# TODO\n",
        "train(BiRNN_attention, pretrained_embedding=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpjn8StjHcY9"
      },
      "source": [
        "## 5. Report (optional)\n",
        "You can briefly report what strategies you attempted in this assignment.\n",
        "\n",
        "- Add weight decay to avoid overfitting\n",
        "- Add Exponential lr schedule strategy"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
